{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>                                     Creation of Bag of Words \n",
    "\n",
    "#### Saved to a file named \"Vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the lemmatiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an Array for StopWords\n",
    "- We are not using the libaries because it doesn't consists of all the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Stopwords \n",
    "stopWords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for reading the .txt files\n",
    "def loadDoc(filename): \n",
    "    file = open(filename, encoding = \"utf8\")                   # open the file as read only\n",
    "    text = file.read()                                         # read all text \n",
    "    file.close()                                               # close the file \n",
    "    return text.lower()                                        # Converting everywords to lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the the html tags\n",
    "def removeTags(raw_html):\n",
    "    cleanr = re.compile('<.*?>')                               # Regex\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method used for removing the non required words/elements.\n",
    "#### Removed are:\n",
    "-\tRemove punctuation.\n",
    "-\tRemove stop words. (We have created 153 words)\n",
    "-\tRemove non-alphabets\n",
    "-\tTokenize all the processed data from above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningProcess(doc):\n",
    "    doc = removeTags(doc)                                      # remove html tags from each token\n",
    "    tokens = doc.split()                                       # Tokenization\n",
    "    \n",
    "    table = str.maketrans('', '', punctuation)                 # Translation table\n",
    "    tokens = [w.translate(table) for w in tokens]              # Process of translating\n",
    "\n",
    "\n",
    "    tokens = [word for word in tokens if word.isalpha()]       # removing non alpha words\n",
    "    tokens = [w for w in tokens if not w in stopWords]         # filter out stop words\n",
    "  \n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]   # Lemmatization process\n",
    "    tokens = [word for word in tokens if len(word) > 1]        # Removing words with length -> 1 (e.g. 'a')\n",
    "\n",
    "    return tokens                                              # Returns a clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def savingListToFile(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def processingDocs(directory, vocab):\n",
    "                                                                # walk through all files in the folder\n",
    "    for filename in listdir(directory):                                           \n",
    "        if not filename.endswith(\".txt\"):                       # skip files that do not have the right extension\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename                       # path -> indicates the specific file path\n",
    "        # add doc to vocab\n",
    "        addingToVocab(path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the Vocab.txt per every run\n",
    "def addingToVocab(filename, vocab):                                                 \n",
    "    doc = loadDoc(filename)                                     # load doc   \n",
    "    tokens = cleaningProcess(doc)                               # clean doc\n",
    "    vocab.update(tokens)                                        # update counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us start building the Vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127675\n",
      "[('movie', 49108), ('film', 45341), ('not', 29765), ('one', 26248), ('like', 19730), ('just', 17309), ('time', 14555), ('good', 14155), ('character', 13692), ('no', 12263), ('get', 12075), ('even', 12048), ('see', 11903), ('story', 11888), ('make', 11771), ('really', 11439), ('can', 11021), ('scene', 10282), ('much', 9387), ('well', 9244), ('great', 9025), ('will', 9013), ('people', 8926), ('bad', 8814), ('also', 8601), ('first', 8431), ('dont', 8353), ('show', 8266), ('way', 8254), ('thing', 8006), ('made', 7713), ('think', 7593), ('go', 7110), ('life', 7104), ('know', 7032), ('love', 6758), ('watch', 6715), ('many', 6670), ('say', 6479), ('seen', 6441), ('plot', 6430), ('actor', 6392), ('two', 6311), ('never', 6296), ('end', 6245), ('look', 6244), ('acting', 6182), ('little', 6145), ('best', 6128), ('year', 6042)]\n",
      "26881\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "processingDocs('C:/Users/jeyam/Machine Learning Project/neg', vocab)    # Preprocessing for negative Reviews\n",
    "processingDocs('C:/Users/jeyam/Machine Learning Project/pos', vocab)    # Preprocessing for positive Reviews\n",
    "print(len(vocab))                                                       # print the size of the vocab\n",
    "print(vocab.most_common(50))                                            # print the top words in the vocab\n",
    "\n",
    "min_occurane = 5                                                        # keep count of tokens which are >= 5 occurrence\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]              # Removing words with no. of occurrence < 5\n",
    "print(len(tokens))\n",
    "\n",
    "savingListToFile(tokens, 'vocab.txt')                                   # save tokens to a vocabulary file                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
